# -*- coding: utf-8 -*-
"""ATOMS_refer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BlfIcub44w7EDlckQ48XMU-zArmXKEHX
"""

import yfinance as yf
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import warnings

warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)

# Fetching AAPL data from Yahoo Finance
df = yf.download('INFY', start='2020-01-01', end='2023-01-01')

# 1. Volatility (ATR - Average True Range)
df['H-L'] = df['High'] - df['Low']
df['H-PC'] = np.abs(df['High'] - df['Close'].shift(1))
df['L-PC'] = np.abs(df['Low'] - df['Close'].shift(1))
df['TR'] = df[['H-L', 'H-PC', 'L-PC']].max(axis=1)
df['ATR'] = df['TR'].rolling(window=14).mean()

# 2. Volume Activity (Volume Compared to Moving Average)
df['Volume_MA'] = df['Volume'].rolling(window=14).mean()
df['Volume_Activity'] = df['Volume'] / df['Volume_MA']

# 3. Price Momentum (ROC - Rate of Change)
df['ROC'] = df['Close'].pct_change(periods=14) * 100

# Drop rows with NaN values
df = df.dropna()

# Select the three factors for clustering
factors = df[['ATR', 'Volume_Activity', 'ROC']]

# Perform KMeans clustering to create 9 clusters
kmeans = KMeans(n_clusters=9, random_state=42)
df['Cluster'] = kmeans.fit_predict(factors)

# Visualize the clusters in a 3D plot
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
scatter = ax.scatter(df['ATR'], df['Volume_Activity'], df['ROC'], c=df['Cluster'], cmap='viridis')

# Set labels
ax.set_xlabel('Volatility (ATR)')
ax.set_ylabel('Volume Activity')
ax.set_zlabel('Price Momentum (ROC)')

# Add legend
legend = ax.legend(*scatter.legend_elements(), title="Clusters")
ax.add_artist(legend)

plt.title('3D Cluster Visualization: AAPL')
plt.show()

"""This code uses KMeans clustering on INFY stock data to identify patterns based on three factors: Volatility (ATR), Volume Activity,
 and Price Momentum (ROC). It fetches historical data, calculates these factors, and then clusters the data into 9 groups.
 A 3D scatter plot visualizes these clusters, showing how different market conditions are represented by the clustering.

"""

import yfinance as yf
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
import plotly.graph_objs as go
import plotly.express as px

# Fetching AAPL data from Yahoo Finance
df = yf.download('INFY', start='2020-01-01', end='2023-01-01')

# 1. Volatility (ATR - Average True Range)
df['H-L'] = df['High'] - df['Low']
df['H-PC'] = np.abs(df['High'] - df['Close'].shift(1))
df['L-PC'] = np.abs(df['Low'] - df['Close'].shift(1))
df['TR'] = df[['H-L', 'H-PC', 'L-PC']].max(axis=1)
df['ATR'] = df['TR'].rolling(window=14).mean()

# 2. Volume Activity (Volume Compared to Moving Average)
df['Volume_MA'] = df['Volume'].rolling(window=14).mean()
df['Volume_Activity'] = df['Volume'] / df['Volume_MA']

# 3. Price Momentum (ROC - Rate of Change)
df['ROC'] = df['Close'].pct_change(periods=14) * 100

# Drop rows with NaN values
df = df.dropna()

# Select the three factors for clustering
factors = df[['ATR', 'Volume_Activity', 'ROC']]

# Perform KMeans clustering to create 9 clusters
kmeans = KMeans(n_clusters=9, random_state=42)
df['Cluster'] = kmeans.fit_predict(factors)

# Plotly 3D scatter plot
fig = px.scatter_3d(df, x='ATR', y='Volume_Activity', z='ROC', color='Cluster',
                    labels={'ATR': 'Volatility (ATR)', 'Volume_Activity': 'Volume Activity', 'ROC': 'Price Momentum (ROC)'},
                    title='3D Cluster Visualization: INFY')

# Show plot
fig.show()

"""This code clusters INFY stock data into 9 groups based on three factors: Volatility (ATR),
Volume Activity, and Price Momentum (ROC). It calculates these metrics from historical data,
performs KMeans clustering, and then uses Plotly to create an interactive 3D scatter plot.
 This visualization helps to see how different market conditions are grouped together.
"""

print

# Define thresholds
momentum_thresholds = pd.qcut(df['ROC'], 3, labels=['Low Momentum', 'Neutral Momentum', 'High Momentum']).astype(str)
volatility_thresholds = pd.qcut(df['ATR'], 3, labels=['Low Volatility', 'Moderate Volatility', 'High Volatility']).astype(str)
volume_thresholds = pd.qcut(df['Volume_Activity'], 3, labels=['Low Volume', 'Average Volume', 'High Volume']).astype(str)

# Assign these thresholds to the dataframe using .loc to avoid the warning
df.loc[:, 'Momentum_Label'] = momentum_thresholds
df.loc[:, 'Volatility_Label'] = volatility_thresholds
df.loc[:, 'Volume_Label'] = volume_thresholds

# Combine these labels into a single category using .loc as well
df.loc[:, 'Combined_Label'] = df['Momentum_Label'] + ', ' + df['Volatility_Label'] + ', ' + df['Volume_Label']

"""This code categorizes the INFY stock data into three levels for each factor:
 Momentum (ROC), Volatility (ATR), and Volume Activity. It uses quantile-based thresholds to
 assign labels ("Low," "Moderate," "High") to these factors. These labels are then combined into a single descriptive category for each data point.
"""

# Calculate the centroid values of each cluster
cluster_centroids = kmeans.cluster_centers_

# Assign labels to clusters based on centroids
cluster_labels = []
for i in range(len(cluster_centroids)):
    momentum = 'High Momentum' if cluster_centroids[i][2] > df['ROC'].quantile(0.67) else ('Low Momentum' if cluster_centroids[i][2] < df['ROC'].quantile(0.33) else 'Neutral Momentum')
    volatility = 'High Volatility' if cluster_centroids[i][0] > df['ATR'].quantile(0.67) else ('Low Volatility' if cluster_centroids[i][0] < df['ATR'].quantile(0.33) else 'Moderate Volatility')
    volume = 'High Volume' if cluster_centroids[i][1] > df['Volume_Activity'].quantile(0.67) else ('Low Volume' if cluster_centroids[i][1] < df['Volume_Activity'].quantile(0.33) else 'Average Volume')

    cluster_labels.append(f'{momentum}, {volatility}, {volume}')

# Map these labels to the clusters in the original dataframe
df['Cluster_Label'] = df['Cluster'].map(dict(enumerate(cluster_labels)))

"""This code calculates the centroid values for each cluster from the KMeans clustering model. It then assigns descriptive labels to each cluster based on the centroid values, using quantiles to determine high, low, or neutral levels for Momentum (ROC), Volatility (ATR), and Volume Activity. These labels are then mapped to each cluster in the original dataframe."""

# Calculate the centroid values of each cluster
cluster_centroids = kmeans.cluster_centers_

# Define the mapping of centroids to the labels
cluster_labels = []
for i in range(len(cluster_centroids)):
    momentum = 'High Momentum' if cluster_centroids[i][2] > df['ROC'].quantile(0.67) else ('Low Momentum' if cluster_centroids[i][2] < df['ROC'].quantile(0.33) else 'Neutral Momentum')
    volatility = 'High Volatility' if cluster_centroids[i][0] > df['ATR'].quantile(0.67) else ('Low Volatility' if cluster_centroids[i][0] < df['ATR'].quantile(0.33) else 'Moderate Volatility')
    volume = 'High Volume' if cluster_centroids[i][1] > df['Volume_Activity'].quantile(0.67) else ('Low Volume' if cluster_centroids[i][1] < df['Volume_Activity'].quantile(0.33) else 'Average Volume')

    cluster_labels.append(f'{momentum}, {volatility}, {volume}')

# Map these labels to the clusters in the original dataframe
df['Cluster_Label'] = df['Cluster'].map(dict(enumerate(cluster_labels)))

# Print out the cluster mappings
cluster_mapping = dict(enumerate(cluster_labels))
cluster_mapping

"""
This code calculates the centroid values for each KMeans cluster and assigns descriptive labels based on these values. The labels categorize clusters into 'High', 'Low', or 'Neutral' for Momentum (ROC), 'High', 'Moderate', or 'Low' for Volatility (ATR), and 'High', 'Average', or 'Low' for Volume Activity. These labels are then mapped to each cluster in the original dataframe. The cluster_mapping dictionary shows the cluster number and its corresponding label."""

momentum_thresholds

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout

# Step 1: Load data
ticker = 'RELIANCE.NS'
data = yf.download(ticker, start="2015-01-01", end="2023-01-01")
close_data = data[['Close']].values

# Step 2: Normalize the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_close_data = scaler.fit_transform(close_data)

# Step 3: Create sequences for prediction (with multi-day predictions)
def create_multi_day_sequences(data, lookback, prediction_days):
    X = []
    y = []
    for i in range(lookback, len(data) - prediction_days):
        X.append(data[i-lookback:i, 0])
        y.append(data[i:i+prediction_days, 0])  # Predict 'prediction_days' ahead
    return np.array(X), np.array(y)

# Modify the number of lookback days and prediction days
lookback_period = 40
prediction_days = 10  # Variable controlling number of days to predict ahead
X, y = create_multi_day_sequences(scaled_close_data, lookback_period, prediction_days)

# Reshape X for LSTM input (samples, timesteps, features)
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# Step 4: Split data into training and testing sets
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Step 5: Build and train the LSTM model
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(lookback_period, 1)))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=prediction_days))  # Output size matches the prediction days

model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=10, batch_size=32)

# Step 6: Generate predictions
predicted_close_prices = model.predict(X_test)
predicted_close_prices = scaler.inverse_transform(predicted_close_prices)  # Reverse scaling

# Get actual close prices for comparison (from the testing set)
actual_close_prices = []
for i in range(len(predicted_close_prices)):
    actual_close_prices.append(close_data[train_size + lookback_period + i:train_size + lookback_period + i + prediction_days, 0])

actual_close_prices = np.array(actual_close_prices)

# Step 7: Plot the actual vs. predicted prices for a sample of days
days_to_plot = 30  # Number of predictions to plot

plt.figure(figsize=(12, 6))
# plt.plot(actual_close_prices[:, 0], color='blue', label='Actual Close Price (Day 1)')
# plt.plot(predicted_close_prices[:, 0], color='red', label='Predicted Close Price (Day 1)')
plt.plot(actual_close_prices[:, -1], color='green', label=f'Actual Close Price (Day {prediction_days})')
plt.plot(predicted_close_prices[:, -1], color='orange', label=f'Predicted Close Price (Day {prediction_days})')
plt.title('Actual vs. Predicted Close Prices (Multi-day)')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()
plt.show()

"""Normalize Data: Scale the prices to a range between 0 and 1 for model training.

Create Sequences: Generate input sequences for the model to predict future prices based on past data.

Split Data: Divide the data into training and testing sets.

Build & Train LSTM Model: Construct an LSTM network with dropout layers to predict prices, and train it on the data.

Generate Predictions: Use the trained model to predict future prices and reverse the scaling to get actual price values.

The strategy uses LSTM to capture sequential dependencies in stock prices, enabling multi-day forecasting. The model is trained, evaluated, and visualized to predict future closing prices, providing a tool for potential investment decisions.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
import yfinance as yf

# Step 1: Load data (5-minute OHLCV data)
ticker = 'AAPL'
data = yf.download(ticker, start="2021-01-01", end="2022-01-01")
df = data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()

# Step 2: Implement momentum indicators

# 1. Rate of Change (ROC)
def calculate_roc(data, n=5):
    return ((data - data.shift(n)) / data.shift(n)) * 100

df['ROC'] = calculate_roc(df['Close'], n=5)

# 2. Relative Strength Index (RSI)
def calculate_rsi(data, periods=14):
    delta = data.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=periods).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=periods).mean()
    rs = gain / loss
    rsi = 100 - (100 / (1 + rs))
    return rsi

df['RSI'] = calculate_rsi(df['Close'], periods=14)

# 3. Momentum Indicator (MOM)
def calculate_momentum(data, n=5):
    return data - data.shift(n)

df['Momentum'] = calculate_momentum(df['Close'], n=5)

# 4. Stochastic Oscillator (%K and %D)
def calculate_stochastic_oscillator(data, k_period=14, d_period=3):
    low_min = data['Low'].rolling(window=k_period).min()
    high_max = data['High'].rolling(window=k_period).max()
    k = 100 * (data['Close'] - low_min) / (high_max - low_min)
    d = k.rolling(window=d_period).mean()
    return k, d

df['%K'], df['%D'] = calculate_stochastic_oscillator(df)

# Step 3: Drop NaN values created by rolling window calculations
df.dropna(inplace=True)

# Step 4: Prepare data for GMM clustering
features = ['ROC', 'RSI', 'Momentum', '%K']  # Momentum indicators for clustering

# Ensure the DataFrame is not empty after dropping NaN values
if df[features].shape[0] > 0:
    # Standardizing the features
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df[features])

    # Step 5: Fit Gaussian Mixture Model (GMM) for two clusters (high vs low momentum)
    gmm = GaussianMixture(n_components=2, random_state=42)  # Two clusters for high/low momentum
    df['Cluster'] = gmm.fit_predict(df_scaled)

    # Step 6: Visualize the clusters
    sns.pairplot(df, vars=features, hue='Cluster', palette='Set1')
    plt.title('High vs Low Momentum Clusters')
    plt.show()

    # Step 7: Analyze the clusters
    cluster_means = df.groupby('Cluster').mean()
    print(cluster_means)

    # Step 8: Classify as high or low momentum
    df['Momentum Type'] = df['Cluster'].apply(lambda x: 'High Momentum' if x == 1 else 'Low Momentum')
    print(df[['Momentum Type', 'ROC', 'RSI', 'Momentum', '%K']].head())
else:
    print("Error: Not enough data after applying momentum indicators. Try adjusting the rolling window periods.")

"""Clustering:
Fits a Gaussian Mixture Model (GMM) with two clusters to identify high and low momentum periods.
Visualizes clusters using pair plots.
Cluster Analysis: Calculates mean values for each cluster and classifies momentum types.
This approach helps identify high and low momentum periods in the stock's price movements.
"""

df

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
import yfinance as yf

# Step 1: Load data (5-minute OHLCV data)
ticker = 'AAPL'
data = yf.download(ticker, start="2020-01-01", end="2022-02-01")
df = data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()

# Step 2: Implement momentum and volatility indicators

# 1. Rate of Change (ROC) for momentum
def calculate_roc(data, n=5):
    return ((data - data.shift(n)) / data.shift(n)) * 100

df['ROC'] = calculate_roc(df['Close'], n=5)

# 2. Average True Range (ATR) for volatility
def calculate_atr(df, n=14):
    high_low = df['High'] - df['Low']
    high_close = np.abs(df['High'] - df['Close'].shift())
    low_close = np.abs(df['Low'] - df['Close'].shift())
    true_range = pd.DataFrame(np.maximum(high_low, np.maximum(high_close, low_close)))
    atr = true_range.rolling(window=n).mean().squeeze()  # Ensure the result is a Series
    return atr

df['ATR'] = calculate_atr(df, n=14)

# Step 3: Drop NaN values created by rolling window calculations
df.dropna(inplace=True)

# Step 4: Prepare data for GMM clustering
features = ['ROC', 'ATR']  # Momentum and volatility indicators for clustering

# Ensure the DataFrame is not empty after dropping NaN values
if df[features].shape[0] > 0:
    # Standardizing the features
    scaler = StandardScaler()
    df_scaled = scaler.fit_transform(df[features])

    # Step 5: Fit Gaussian Mixture Model (GMM) for four clusters
    gmm = GaussianMixture(n_components=4, random_state=42)  # Four clusters for various momentum and volatility conditions
    df['Cluster'] = gmm.fit_predict(df_scaled)

    # Step 6: Visualize the clusters
    sns.pairplot(df, vars=features, hue='Cluster', palette='Set1')
    plt.title('Momentum-Volatility Clusters')
    plt.show()

    # Step 7: Analyze the clusters
    cluster_means = df.groupby('Cluster').mean()
    print(cluster_means)

    # Step 8: Label the clusters based on conditions
    conditions = {
        0: 'High Momentum, High Volatility',
        1: 'Low Momentum, Low Volatility',
        2: 'Low Momentum, High Volatility',
        3: 'High Momentum, Low Volatility'
    }

    # Assigning descriptive labels to each cluster
    df['Cluster Label'] = df['Cluster'].map(conditions)
    print(df[['Cluster Label', 'ROC', 'ATR']].head())

else:
    print("Error: Not enough data after applying momentum and volatility indicators. Try adjusting the rolling window periods.")

"""The code clusters Apple stock data using momentum (ROC) and volatility (ATR) indicators. It fetches historical data, calculates these indicators, and uses Gaussian Mixture Models (GMM) to classify stock behavior into four groups. The clusters are labeled based on combinations of high/low momentum and volatility."""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import yfinance as yf

# Load data
ticker = 'AAPL'
data = yf.download(ticker, start="2018-01-01", end="2023-01-01", interval='1d')
df = data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()

# Calculate Moving Averages
def calculate_moving_averages(df, short_window=50, long_window=200):
    df['MA_Short'] = df['Close'].rolling(window=short_window, min_periods=1).mean()
    df['MA_Long'] = df['Close'].rolling(window=long_window, min_periods=1).mean()
    return df

df = calculate_moving_averages(df)

# Create Labels
def create_labels(df):
    df['Market Trend'] = np.where(df['Close'] > df['MA_Short'], 'Bullish', 'Bearish')
    df['Market Trend'] = np.where(df['Close'] < df['MA_Long'], 'Bearish', df['Market Trend'])
    df['Market Trend'] = np.where((df['Close'] <= df['MA_Short']) & (df['Close'] >= df['MA_Long']), 'Sideways', df['Market Trend'])
    return df

df = create_labels(df)

# Prepare data for KNN
features = ['Close', 'MA_Short', 'MA_Long']
X = df[features].copy()  # Ensure X is a DataFrame
y = df['Market Trend'].copy()

# Drop rows with missing values
X.dropna(inplace=True)
y = y.loc[X.index]  # Ensure y matches X's index

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Train KNN Classifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Predict and evaluate
y_pred = knn.predict(X_test)

# Convert test predictions and actual values to DataFrame for easy comparison
test_index = X_test.index if hasattr(X_test, 'index') else np.arange(len(X_test))  # Handle if X_test is ndarray
df_test_predictions = pd.DataFrame({
    'Actual': y_test.reindex(test_index).values,
    'Predicted': y_pred
}, index=test_index)

print(df_test_predictions.head())

# Plot data and predictions
plt.figure(figsize=(14, 7))
plt.plot(df.index, df['Close'], label='Close Price', color='black', alpha=0.5)
plt.plot(df.index, df['MA_Short'], label='50-Day MA', color='blue', linestyle='--')
plt.plot(df.index, df['MA_Long'], label='200-Day MA', color='red', linestyle='--')

# Marking the classifications
colors = {'Bullish': 'green', 'Bearish': 'red', 'Sideways': 'gray'}
plt.scatter(df.index, df['Close'], c=df['Market Trend'].apply(lambda x: colors[x]), label='Market Trend', alpha=0.6)

plt.title('Stock Price and Market Trend Classification with KNN')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show()

"""The code uses K-Nearest Neighbors (KNN) to classify Apple stock market trends into "Bullish," "Bearish," or "Sideways" based on moving averages. It fetches historical stock data, calculates short-term (50-day) and long-term (200-day) moving averages, and assigns trend labels. The data is standardized and split
into training and testing sets. The KNN model is trained on the data, and predictions are compared against actual trends. Finally, it plots the stock prices, moving averages, and market trends for visualization.
"""

import pandas as pd
import numpy as np
import yfinance as yf
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load data
ticker = 'AAPL'
data = yf.download(ticker, start="2018-01-01", end="2023-01-01", interval='1d')
df = data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()

# Calculate Indicators
def calculate_indicators(df):
    # ATR Calculation
    df['ATR'] = df[['High', 'Close']].apply(lambda x: max(x['High'] - x['Close'], 0), axis=1).rolling(window=14).mean()

    # Rate of Change (ROC)
    df['ROC'] = ((df['Close'] - df['Close'].shift(5)) / df['Close'].shift(5)) * 100

    # Stochastic Oscillator (%K and %D)
    def stochastic_oscillator(df, k_period=14, d_period=3):
        low_min = df['Low'].rolling(window=k_period).min()
        high_max = df['High'].rolling(window=k_period).max()
        df['%K'] = 100 * (df['Close'] - low_min) / (high_max - low_min)
        df['%D'] = df['%K'].rolling(window=d_period).mean()

    stochastic_oscillator(df)

calculate_indicators(df)

# Drop NaN values
df.dropna(inplace=True)

# Prepare data for KMeans clustering
features = ['Close', 'ATR', 'ROC', '%K', '%D']
X = df[features].copy()

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply KMeans Clustering
kmeans = KMeans(n_clusters=4, random_state=42)  # 4 clusters: Momentum, Volatile, Mean Reversion, None
df['Cluster'] = kmeans.fit_predict(X_scaled)

# Analyze clusters
def analyze_clusters(df):
    cluster_means = df.groupby('Cluster').mean()
    print("Cluster Means:")
    print(cluster_means)
    return cluster_means

cluster_means = analyze_clusters(df)

# Calculate the median for each feature from the entire dataset
atr_median = df['ATR'].median()
roc_median = df['ROC'].median()

# Assign Labels to Clusters
def assign_labels(df, cluster_means, atr_median, roc_median):
    # Example logic to label clusters based on feature means
    cluster_labels = {}
    for cluster in cluster_means.index:
        mean_values = cluster_means.loc[cluster]

        if mean_values['ROC'] > roc_median and mean_values['ATR'] > atr_median:
            cluster_labels[cluster] = 'Momentum'
        elif mean_values['ATR'] > atr_median:
            cluster_labels[cluster] = 'Volatile'
        elif mean_values['ROC'] < roc_median and mean_values['ATR'] < atr_median:
            cluster_labels[cluster] = 'Mean Reversion'
        else:
            cluster_labels[cluster] = 'None'

    df['Market Condition'] = df['Cluster'].map(cluster_labels)
    return df, cluster_labels

df, cluster_labels = assign_labels(df, cluster_means, atr_median, roc_median)

# Plot results
plt.figure(figsize=(14, 7))
plt.scatter(df.index, df['Close'], c=df['Cluster'], cmap='viridis', label='Clusters')
plt.colorbar(label='Cluster')
plt.title('Stock Price and Market Conditions Clustering')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show()

# Print final data with assigned labels
print(df[['Close', 'ATR', 'ROC', '%K', '%D', 'Cluster', 'Market Condition']].head())

# Print cluster labels
print("Cluster Labels:")
for cluster, label in cluster_labels.items():
    print(f"Cluster {cluster}: {label}")

"""
The script analyzes AAPL stock by calculating indicators (ATR, ROC, Stochastic Oscillator) and applying KMeans clustering to identify market conditions (Momentum, Volatile, Mean Reversion, None). It labels each cluster based on indicator medians, visualizes the clusters, and prints the resulting market condition classifications."""

import pandas as pd
import numpy as np
import yfinance as yf
import matplotlib.pyplot as plt

# Load data
ticker = 'AAPL'
data = yf.download(ticker, start="2018-01-01", end="2023-01-01", interval='1d')
df = data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()

# Calculate Indicators
def calculate_indicators(df):
    # ATR Calculation (Volatility Indicator)
    df['ATR'] = df[['High', 'Close']].apply(lambda x: max(x['High'] - x['Close'], 0), axis=1).rolling(window=14).mean()

    # Rate of Change (ROC - Momentum Indicator)
    df['ROC'] = ((df['Close'] - df['Close'].shift(5)) / df['Close'].shift(5)) * 100

calculate_indicators(df)

# Drop NaN values
df.dropna(inplace=True)

# Calculate medians for ATR and ROC
atr_median = df['ATR'].median()
roc_median = df['ROC'].median()

# Classify data based on momentum and volatility
def classify_momentum_volatility(df, atr_median, roc_median):
    df['Momentum'] = np.where(df['ROC'] > roc_median, 'High Momentum', 'Low Momentum')
    df['Volatility'] = np.where(df['ATR'] > atr_median, 'High Volatility', 'Low Volatility')

    return df

df = classify_momentum_volatility(df, atr_median, roc_median)

# Plot the classifications
plt.figure(figsize=(14, 7))
high_momentum = df[df['Momentum'] == 'High Momentum']
low_momentum = df[df['Momentum'] == 'Low Momentum']

plt.scatter(high_momentum.index, high_momentum['Close'], color='green', label='High Momentum', alpha=0.6)
plt.scatter(low_momentum.index, low_momentum['Close'], color='red', label='Low Momentum', alpha=0.6)
plt.title('Classification Based on Momentum')
plt.xlabel('Date')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show()

# Print sample classified data
print(df[['Close', 'ATR', 'ROC', 'Momentum', 'Volatility']].head())

"""The script analyzes AAPL stock data from 2018 to 2023 by calculating two indicators: ATR (Average True Range for volatility) and ROC (Rate of Change for momentum). It then classifies each day as either High/Low Momentum and High/Low Volatility based on the median values of these indicators. The classified data is visualized with scatter plots showing price movements according to momentum classifications."""

pip install tslearnfrom
tslearn.clustering import TimeSeriesKMeans
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import scipy
import sklearn.preprocessing
import yfinance as yf
p=yf.download("ABB.NS","2017-01-01","2023-01-01",)
model=TimeSeriesKMeans(n_clusters=4,metric="dtw",max_iter=20,n_init=10)
alpha=(p['Close'].rolling(window=12,min_periods=1).std()).to_numpy()
alpha=alpha.reshape(-1,1)
alpha_2=(p['Close']-p['Open'])/(p['Close'])
alpha = np.where(np.isinf(alpha), 1, alpha)
alpha=np.where(np.isnan(alpha),0,alpha)
model.fit(alpha)
model.cluster_centers_
model.labels_
p['alpha_1']=model.labels_
#DEFINING PARTS
p_alpha_0=p[p['alpha_1']==0]
p_alpha_1=p[p['alpha_1']==1]
p_alpha_2=p[p['alpha_1']==2]
p_alpha_3=p[p['alpha_1']==3]
plt.figure(figsize=(20,15))
plt.plot(p.index.values,p['Close'])
plt.scatter(p_alpha_0.index.values,p_alpha_0['Close'])
plt.scatter(p_alpha_1.index.values,p_alpha_1['Close'])
plt.scatter(p_alpha_2.index.values,p_alpha_2['Close'])
plt.scatter(p_alpha_3.index.values,p_alpha_3['Close'])

# Fetch data
p = yf.download("ABB.NS", "2017-01-01", "2023-01-01")

# Tenkan-sen (Conversion Line)
p['tenkan_sen'] = (p['High'].rolling(window=9).max() + p['Low'].rolling(window=9).min()) / 2

# Kijun-sen (Base Line)
p['kijun_sen'] = (p['High'].rolling(window=26).max() + p['Low'].rolling(window=26).min()) / 2

# Senkou Span A (Leading Span A)
p['senkou_span_a'] = ((p['tenkan_sen'] + p['kijun_sen']) / 2).shift(26)

# Senkou Span B (Leading Span B)
p['senkou_span_b'] = (p['High'].rolling(window=52).max() + p['Low'].rolling(window=52).min()) / 2
p['senkou_span_b'] = p['senkou_span_b'].shift(26)

# Chikou Span (Lagging Span)
p['chikou_span'] = p['Close'].shift(-26)

# Inspect the first few rows of the Ichimoku Cloud parameters
print(p[['tenkan_sen', 'kijun_sen', 'senkou_span_a', 'senkou_span_b', 'chikou_span']].head(20))

"""Clustering: Uses Time Series K-Means with DTW to cluster the rolling standard deviation of closing prices (alpha) into four groups, visualizing them on the price chart.

Ichimoku Cloud: Calculates key Ichimoku indicators (Tenkan-sen, Kijun-sen, Senkou Span A/B, Chikou Span) and prints their values for further analysis.
"""

def hurst_f(input_ts, lags_to_test=20):
            # interpretation of return value
            # hurst < 0.5 - input_ts is mean reverting
            # hurst = 0.5 - input_ts is effectively random/geometric brownian motion
            # hurst > 0.5 - input_ts is trending
            tau = []
            lagvec = []
            #  Step through the different lags
            for lag in range(2, lags_to_test):
                #  produce price difference with lag
                pp = np.subtract(input_ts[lag:].values, input_ts[:-lag].values)
                #  Write the different lags into a vector
                lagvec.append(lag)
                #  Calculate the variance of the differnce vector
                tau.append(np.sqrt(np.std(pp)))
            #  linear fit to double-log graph (gives power)
            m = np.polyfit(np.log10(lagvec), np.log10(tau), 1)
            # calculate hurst
            hurst = m[0]*2
            print(hurst)
            return hurst

"""The hurst_f function calculates the Hurst exponent of a time series to identify its behavior:

Hurst < 0.5: Mean-reverting
Hurst = 0.5: Random (like a random walk)
Hurst > 0.5: Trending
It computes the differences at various lags, fits a line to the log-log plot of lag vs. variance, and uses the slope to determine the Hurst exponent. The result indicates whether the series tends to revert, is random, or trends.
"""

!pip install yfinance
!pip install tslearn
!pip install hmmlearn
import yfinance as yf
from datetime import date
import numpy as np
import pandas as pd
from hmmlearn import hmm
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [16, 12]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.graph_objs as go
from tslearn.clustering import TimeSeriesKMeans
from datetime import date

def ichimoku_clustering_analysis(df, n_clusters=4, column_mapping=None, ichimoku_weight=0.8, clustering_weight=0.2):
    # Ensure the column mapping is provided
    if column_mapping is None:
        column_mapping = {
            'High': 'High',
            'Low': 'Low',
            'Close': 'Close'
        }

    # Compute Ichimoku components
    high_9 = df[column_mapping['High']].rolling(window=9).max()
    low_9 = df[column_mapping['Low']].rolling(window=9).min()
    tenkan_sen = (high_9 + low_9) / 2

    high_26 = df[column_mapping['High']].rolling(window=26).max()
    low_26 = df[column_mapping['Low']].rolling(window=26).min()
    kijun_sen = (high_26 + low_26) / 2

    senkou_span_a = ((tenkan_sen + kijun_sen) / 2).shift(26)
    high_52 = df[column_mapping['High']].rolling(window=52).max()
    low_52 = df[column_mapping['Low']].rolling(window=52).min()
    senkou_span_b = ((high_52 + low_52) / 2).shift(26)

    ema_12 = df[column_mapping['Close']].ewm(span=12, adjust=False).mean()
    ema_26 = df[column_mapping['Close']].ewm(span=26, adjust=False).mean()
    macd = ema_12 - ema_26
    macd_signal = macd.ewm(span=9, adjust=False).mean()

    # Determine market conditions: bullish, bearish, and sideways
    bullish = (df[column_mapping['Close']] > senkou_span_a) & (df[column_mapping['Close']] > senkou_span_b) & (tenkan_sen > kijun_sen) & (macd > macd_signal)
    bearish = (df[column_mapping['Close']] < senkou_span_a) & (df[column_mapping['Close']] < senkou_span_b) & (tenkan_sen < kijun_sen) & (macd < macd_signal)
    sideways = ~(bullish | bearish)  # If neither bullish nor bearish, it's sideways

    trend_classification_ichimoku = np.where(bullish, 1, np.where(bearish, 0, 2))


    # Feature Engineering
    p = reliance.copy()
    p['return'] = p['Close'].pct_change()  # Price returns
    p['momentum'] = p['Close'] - p['Close'].shift(10)  # Momentum over 10 days
    p['short_ma'] = p['Close'].rolling(window=20).mean()  # 20-day moving average
    p['long_ma'] = p['Close'].rolling(window=50).mean()  # 50-day moving average
    p['trend'] = p['short_ma'] - p['long_ma']  # Trend difference
    p['volatility'] = p['Close'].rolling(window=26).std()  # Volatility
    p['rsi'] = 100 - (100 / (1 + p['Close'].diff(1).apply(lambda x: max(x, 0)).rolling(window=14).mean() / p['Close'].diff(1).apply(lambda x: abs(min(x, 0))).rolling(window=14).mean()))  # RSI
    p['VIX']=vix_data['Close']
    p['TR'] = np.maximum(p['High'] - p['Low'],
                     np.maximum(abs(p['High'] - p['Close'].shift(1)),
                                abs(p['Low'] - p['Close'].shift(1))))
    p['ATR'] = p['TR'].rolling(window=14).mean()
    p['upper_wick'] = p['High'] - p[['Open', 'Close']].max(axis=1)
    p['lower_wick'] = p[['Open', 'Close']].min(axis=1) - p['Low']
    p['wick_to_body'] = (p['upper_wick'] + p['lower_wick']) / (p['High']-p['Low'])
    p['volume_spike'] = np.where(p['Volume'] > 1.5 * p['Volume'].rolling(window=20).mean(),1,0)

    # Combine features into an array
    features = np.column_stack([p['VIX'].fillna(0), p['wick_to_body'].fillna(0), p['trend'].fillna(0), p['ATR'].fillna(0), p['rsi'].fillna(50),p['volume_spike'].fillna(p['volume_spike'].mean()) ])

    features = np.array(features, dtype=np.float64)  # Convert to float64 to ensure numeric

    # If there are NaNs in the data, handle them (e.g., fill with a value or remove)
    features = np.nan_to_num(features)  # Replace NaNs with 0 (or any other value)

    # Convert all data in the features DataFrame to numeric, coercing errors to NaN
    # features = features.apply(pd.to_numeric, errors='coerce')

    # Now apply the StandardScaler
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)

    # TimeSeriesKMeans Clustering
    model = TimeSeriesKMeans(n_clusters=3, metric="dtw", max_iter=20, n_init=5)
    model.fit(features_scaled)
    p['trend_cluster'] = model.labels_

    # Assign clusters to different trends (you might want to check the actual labels by visual inspection)
    p_1 = p[p['trend_cluster'] == 0]
    p_2 = p[p['trend_cluster'] == 1]
    p_3 = p[p['trend_cluster'] == 2]
    p_4 = p[p['trend_cluster'] == 3]
    p_5 = p[p['trend_cluster'] == 4]

    # Plotting the results
    plt.figure(figsize=(20,15))
    plt.scatter(p_1.index.values, p_1['Close'], color='red', label='1')
    plt.scatter(p_2.index.values, p_2['Close'], color='green', label='2')
    plt.scatter(p_3.index.values, p_3['Close'], color='yellow', label='3')
    plt.scatter(p_4.index.values, p_4['Close'], color='blue', label='4')
    plt.scatter(p_5.index.values, p_5['Close'], color='black', label='5')


     # Time Series Clustering
    model = TimeSeriesKMeans(n_clusters=n_clusters, metric="dtw", verbose=False, random_state=42)
    clusters = model.fit_predict(features)

    x = df.index  # This could be the date or any other index you want on the x-axis
    y = clusters  # This should be the result of your clustering or another y variable

    # Now plot the scatter plot
    plt.scatter(x, y, c=y, cmap='viridis')  # c=y for color-coding based on clusters
    plt.title('Clustering of Reliance Stock Price into Bearish, Bullish, and Sideways Trends')
    plt.xlabel('Date')
    plt.ylabel('Cluster Labels')
    plt.legend()
    plt.show()

    # Calculate barycenters of the clusters
    barycenters = model.cluster_centers_

    # Calculate the weighted average trend classification
    trend_classification = (ichimoku_weight * trend_classification_ichimoku + clustering_weight * clusters).round().astype(int)

    # Plotting the Ichimoku Cloud and other relevant graphs
    plt.figure(figsize=(16, 10))

    # Price plot
    plt.plot(df.index, df[column_mapping['Close']], label='Close Price')
    plt.plot(df.index, tenkan_sen, label='Tenkan Sen', linestyle='--')
    plt.plot(df.index, kijun_sen, label='Kijun Sen', linestyle='--')
    plt.fill_between(df.index, senkou_span_a, senkou_span_b, where=senkou_span_a >= senkou_span_b, color='lightgreen', alpha=0.4, label='Senkou Span A')
    plt.fill_between(df.index, senkou_span_a, senkou_span_b, where=senkou_span_a < senkou_span_b, color='lightcoral', alpha=0.4, label='Senkou Span B')
    plt.plot(df.index, senkou_span_a, label='Senkou Span A', linestyle='--')
    plt.plot(df.index, senkou_span_b, label='Senkou Span B', linestyle='--')
    plt.plot(df.index, macd, label='MACD', linestyle='--')
    plt.plot(df.index, macd_signal, label='MACD Signal', linestyle='--')

    plt.title('Ichimoku Cloud & MACD')
    plt.legend()
    plt.show()

    return clusters, barycenters
start_date = '2017-01-01'
end_date = '2023-01-01'
reliance = yf.download('RELIANCE.NS', start=start_date, end=end_date)


vix_symbol = "^VIX"
vix_data = yf.download(vix_symbol, start_date, end_date)
trend,centers=ichimoku_clustering_analysis(reliance, n_clusters=4, column_mapping=None, ichimoku_weight=0.5, clustering_weight=0.5)
def segment_and_majority_vote(arr, segment_length=20, threshold=0.7):

    segmented_arr = np.copy(arr)

    for i in range(0, len(arr), segment_length):
        # Extract the current segment
        segment = arr[i:i + segment_length]

        # Find the majority group in the segment
        values, counts = np.unique(segment, return_counts=True)
        max_count = np.max(counts)
        majority_value = values[np.argmax(counts)]

        # Check if the majority value exceeds the threshold
        if max_count / len(segment) >= threshold:
            # Set all elements in the segment to the majority value
            segmented_arr[i:i + segment_length] = majority_value

    return segmented_arr


np.set_printoptions(threshold=np.inf)

"""This code analyzes stock trends using Ichimoku Cloud, MACD, and time-series KMeans clustering. It calculates Ichimoku indicators (Tenkan Sen, Kijun Sen, Senkou Span A/B) and technical features like returns, RSI, ATR, VIX, and volatility. Clustering is applied to segment market conditions (bullish, bearish, sideways). A majority voting method stabilizes trend classifications over time. The results are visualized with plots showing stock prices, Ichimoku Cloud components, and cluster labels, providing insight into market behavior.

"""

# print(final_trend)
print(trend)
print(centers)

from tslearn.metrics import dtw
from tslearn.clustering import TimeSeriesKMeans
import numpy as np
import yfinance as yf

def predict_cluster(new_data, centers):
    # Extract and preprocess features from new_data
    features = np.column_stack([
        new_data['VIX'],
        new_data['wick_to_body'],
        new_data['trend'],
        new_data['ATR'],
        new_data['rsi'],
        new_data['volume_spike']
    ])

    features = features.reshape(-1, 1)
    features = np.nan_to_num(features)

    distances = [dtw(features, center) for center in centers]
    predicted_cluster = np.argmin(distances)
    return predicted_cluster

def find_category(data, centers):
    # Feature engineering for new data
    p = data.copy()
    p['return'] = p['Close'].pct_change()
    p['momentum'] = p['Close'] - p['Close'].shift(10)
    p['short_ma'] = p['Close'].rolling(window=20).mean()
    p['long_ma'] = p['Close'].rolling(window=50).mean()
    p['trend'] = p['short_ma'] - p['long_ma']
    p['volatility'] = p['Close'].rolling(window=26).std()
    p['rsi'] = 100 - (100 / (1 + p['Close'].diff(1).apply(lambda x: max(x, 0)).rolling(window=14).mean() /
                               p['Close'].diff(1).apply(lambda x: abs(min(x, 0))).rolling(window=14).mean()))
    p['VIX'] = vix_data['Close']
    p['TR'] = np.maximum(p['High'] - p['Low'],
                         np.maximum(abs(p['High'] - p['Close'].shift(1)),
                                    abs(p['Low'] - p['Close'].shift(1))))
    p['ATR'] = p['TR'].rolling(window=14).mean()
    p['upper_wick'] = p['High'] - p[['Open', 'Close']].max(axis=1)
    p['lower_wick'] = p[['Open', 'Close']].min(axis=1) - p['Low']
    p['wick_to_body'] = (p['upper_wick'] + p['lower_wick']) / (p['High'] - p['Low'])
    p['volume_spike'] = np.where(p['Volume'] > 1.5 * p['Volume'].rolling(window=20).mean(), 1, 0)

    # Predict cluster for each data point
    pred_arr = []
    for i in range(52, len(p)):
        pred_arr.append(predict_cluster(p.iloc[i], centers))

    return pred_arr

# Fetch and process data
data = yf.download("BHARTIARTL.NS", '2017-01-01', '2022-01-01')
vix_data = yf.download("VIX", '2017-01-01', '2022-01-01')  # Example VIX data

# Define your features and apply clustering
features = np.column_stack([
    data['Close'].pct_change(),
    (data['Close'] - data['Close'].shift(10)),
    data['Close'].rolling(window=20).mean() - data['Close'].rolling(window=50).mean(),
    data['Close'].rolling(window=26).std(),
    100 - (100 / (1 + data['Close'].diff(1).apply(lambda x: max(x, 0)).rolling(window=14).mean() /
                   data['Close'].diff(1).apply(lambda x: abs(min(x, 0))).rolling(window=14).mean())),
    np.maximum(data['High'] - data['Low'],
               np.maximum(abs(data['High'] - data['Close'].shift(1)),
                          abs(data['Low'] - data['Close'].shift(1)))),
    (data['High'] - data[['Open', 'Close']].max(axis=1) + data[['Open', 'Close']].min(axis=1) - data['Low']) /
    (data['High'] - data['Low']),
    np.where(data['Volume'] > 1.5 * data['Volume'].rolling(window=20).mean(), 1, 0)
])

features = features[52:]  # Adjust for the initial period without complete data

# Clustering
model = TimeSeriesKMeans(n_clusters=4, metric="dtw", max_iter=20, n_init=10)
model.fit(features)
centers = model.cluster_centers_

# Run the classification
predicted_clusters = find_category(data, centers)
print(predicted_clusters)

"""
This code applies time-series clustering using Dynamic Time Warping (DTW) and TimeSeriesKMeans to stock data (Bharti Airtel and VIX). It calculates technical features such as RSI, ATR, momentum, trend, and volatility, then clusters the data into four market behavior groups. The function find_category predicts the cluster for new data points by comparing them to cluster centers using DTW distance. The code fetches stock data, performs feature engineering, and assigns each data point to a cluster based on the similarity of its features to the cluster centers
"""